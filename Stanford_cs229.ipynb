{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf8ed19c-c7d8-41de-8316-f0ea9630ea46",
   "metadata": {},
   "source": [
    "https://lena-voita.github.io/nlp_course/language_modeling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40dd7ed-83bd-4dd3-b46c-ec5f3f1f6aef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T12:23:20.212534Z",
     "iopub.status.busy": "2024-11-21T12:23:20.211984Z",
     "iopub.status.idle": "2024-11-21T12:23:20.219996Z",
     "shell.execute_reply": "2024-11-21T12:23:20.218621Z",
     "shell.execute_reply.started": "2024-11-21T12:23:20.212490Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2959ec59-2af1-4f29-97a7-783ec1d3c966",
   "metadata": {},
   "source": [
    "- what matters when training LLMs\n",
    "    - Architecture\n",
    "    - Training Algorithms/loss\n",
    "    - Data\n",
    "    - Evaluation\n",
    "    - Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5287a220-a66e-4ea1-9d3c-3fb5188af75d",
   "metadata": {},
   "source": [
    "### language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc3b6a-fc85-45ff-87ce-92e85158ef2b",
   "metadata": {},
   "source": [
    "- language modeling\n",
    "    - probability distribution over sequences of tokens/words $p(x_1,\\cdots, p_L)$\n",
    "- LMs are generative models: $x_{1:L}\\sim p(x_1,\\cdots,x_L)$\n",
    "    - 因为语言模型是一个概率分布，生成就是从分布中采样 sampling；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4548a7f-8db9-4351-ab9c-8d1b125a5049",
   "metadata": {},
   "source": [
    "### autoregressive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe4e512-5596-4a26-a104-e139eac7d5c4",
   "metadata": {},
   "source": [
    "- Autoregressive (AR) language models\n",
    "    - $p(x_1,\\cdots, x_L)=p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)\\cdots$\n",
    "    - 注意，这个不是近似，而是 chain rule of probability\n",
    "    - you only need a model that can predict the next token given past context.\n",
    "- tasks & steps\n",
    "    - task: predict the next word\n",
    "    - steps:\n",
    "        - tokenize\n",
    "        - forward\n",
    "        - **predict probability of next token**\n",
    "        - sample\n",
    "        - detokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f45272-766f-4277-955e-6bc24ae18c47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T12:23:36.146481Z",
     "iopub.status.busy": "2024-11-21T12:23:36.145879Z",
     "iopub.status.idle": "2024-11-21T12:23:36.158230Z",
     "shell.execute_reply": "2024-11-21T12:23:36.155987Z",
     "shell.execute_reply.started": "2024-11-21T12:23:36.146436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://lena-voita.github.io/resources/lectures/lang_models/neural/nn_lm_idea_linear-min.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://lena-voita.github.io/resources/lectures/lang_models/neural/nn_lm_idea_linear-min.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c541b123-00f1-42fa-9e5a-f1dcb25b36c6",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24b6fa98-d8d3-481a-bcaf-f3712d72989d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T12:24:32.188052Z",
     "iopub.status.busy": "2024-11-21T12:24:32.187477Z",
     "iopub.status.idle": "2024-11-21T12:24:32.199860Z",
     "shell.execute_reply": "2024-11-21T12:24:32.197957Z",
     "shell.execute_reply.started": "2024-11-21T12:24:32.188006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://lena-voita.github.io/resources/lectures/lang_models/neural/one_step_loss_intuition-min.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://lena-voita.github.io/resources/lectures/lang_models/neural/one_step_loss_intuition-min.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59949bf-8be5-481a-9cb5-59f2f2323fda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T12:26:36.199362Z",
     "iopub.status.busy": "2024-11-21T12:26:36.198737Z",
     "iopub.status.idle": "2024-11-21T12:26:36.212057Z",
     "shell.execute_reply": "2024-11-21T12:26:36.209455Z",
     "shell.execute_reply.started": "2024-11-21T12:26:36.199318Z"
    }
   },
   "source": [
    "- maximize text's log-likelihood = minimum the cross entropy loss\n",
    "\n",
    "$$\n",
    "\\max \\prod_{i} p(x_i | x_{1:i-1}) = \\min \\left( - \\sum_{i} \\log p(x_i | x_{1:i-1}) \\right) = \\min \\mathcal{L}(x_{1:L})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c4996-42af-4289-8f1c-255477f95cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
