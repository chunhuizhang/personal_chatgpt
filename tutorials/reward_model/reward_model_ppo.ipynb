{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f3fee5-fd95-4008-a45a-6668b030d076",
   "metadata": {},
   "source": [
    "- https://pub.aimind.so/reinforcement-learning-meets-large-language-models-llms-aligning-human-preferences-in-llms-88c3a3f1a3f9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4056257f-8947-4482-a948-b288375391f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T15:19:37.918352Z",
     "iopub.status.busy": "2024-11-11T15:19:37.917734Z",
     "iopub.status.idle": "2024-11-11T15:19:37.929219Z",
     "shell.execute_reply": "2024-11-11T15:19:37.927451Z",
     "shell.execute_reply.started": "2024-11-11T15:19:37.918305Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc0faba-9732-4ace-a305-329d61515fbd",
   "metadata": {},
   "source": [
    "- We define $r_\\theta(x, y)$ as the reward model being trained, parameterized by $\\theta$.\n",
    "- Where you can imagine that in the above equation, x is the prompt (state) given to the LLMs agent, and y is the completion (action) given by LLM:\n",
    "    - $S_t=x$\n",
    "    - $a_t=y$\n",
    "- LLM as the RL policy\n",
    "    - action: generate tokens\n",
    "    - a {prompt, completion} pair => rm => reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d608ff5b-c36f-49c4-97aa-fc1c5eb13b9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T15:20:44.304652Z",
     "iopub.status.busy": "2024-11-11T15:20:44.304019Z",
     "iopub.status.idle": "2024-11-11T15:20:44.316211Z",
     "shell.execute_reply": "2024-11-11T15:20:44.314132Z",
     "shell.execute_reply.started": "2024-11-11T15:20:44.304604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*WzmPShlUbdZwdBO65OtrMg.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://miro.medium.com/v2/resize:fit:4800/format:webp/1*WzmPShlUbdZwdBO65OtrMg.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9879fae2-da67-496a-b6a5-fbe64fbe0ca4",
   "metadata": {},
   "source": [
    "Having defined the objective function (above), we can use Proximal Policy Optimization (PPO) as a reinforcement learning algorithm. The main idea of PPO is to compute the gradient of the objective function with respect to the “policy” parameter (ϕ), is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d5b89fe-b340-4ae4-b879-4b3d84eff8fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T15:21:09.894126Z",
     "iopub.status.busy": "2024-11-11T15:21:09.893538Z",
     "iopub.status.idle": "2024-11-11T15:21:09.905948Z",
     "shell.execute_reply": "2024-11-11T15:21:09.903896Z",
     "shell.execute_reply.started": "2024-11-11T15:21:09.894080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*vbnhEy2GFJPLfIP83LvuHQ.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://miro.medium.com/v2/resize:fit:640/format:webp/1*vbnhEy2GFJPLfIP83LvuHQ.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e3239-668e-4140-95ef-f8a28af8a962",
   "metadata": {},
   "source": [
    "### PPO with clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0459015d-c3b2-43fd-8a4a-3dcae3f1d987",
   "metadata": {},
   "source": [
    "- https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo\n",
    "- https://huggingface.co/learn/deep-rl-course/unit8/clipped-surrogate-objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0911ce7b-8f5c-4253-b2e3-43e5dee5b456",
   "metadata": {},
   "source": [
    "为了确保训练的稳定性并防止策略发生过大的更新，可以使用**近端策略优化（PPO）**算法。PPO 的目标函数引入了一个**截断的代理损失**：\n",
    "\n",
    "$$\n",
    "L^\\text{PPO}(\\theta) = \\mathbb{E}_{x \\sim D, y \\sim \\pi_\\theta} \\left[ \\min\\left( r_\\theta(y \\mid x) A(y, x), \\text{clip}\\left(r_\\theta(y \\mid x), 1 - \\epsilon, 1 + \\epsilon\\right) A(y, x) \\right) \\right]\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $ r_\\theta(y \\mid x) = \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{\\theta_{\\text{old}}}(y \\mid x)} $ 是**概率比值**。\n",
    "- $ A(y, x) = R_\\phi(y) - V_{\\theta_{\\text{old}}}(x)$ 是**优势函数**。\n",
    "- $ \\epsilon $ 是控制截断范围的超参数。\n",
    "- $ V_{\\theta_{\\text{old}}}(x) $ 是估计给定 $ x $ 的期望奖励的**值函数**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed7fff-84d9-4873-8b4f-fb19ef41ea63",
   "metadata": {},
   "source": [
    "#### 奖励函数与值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b33bfc-73a1-4c39-ad44-777b5a43ae7b",
   "metadata": {},
   "source": [
    "- 定义： 奖励函数 $R(s,a)$ 定义了在状态 $s$ 下执行动作 $a$ 后获得的即时奖励。\n",
    "    - 在 RLHF 中的应用： 在大型语言模型的训练中，奖励函数通常表示为 $R(y)$，用于评估模型生成的回复 $y$ 的质量。这里，奖励模型 $R_\\phi(y)$ 已通过人类偏好数据进行训练，能够为每个回复 $y$ 提供一个标量奖励。\n",
    "- 值函数 $V(s)$ 表示在状态 $s$ 下，遵循策略 $\\pi$ 所能获得的预期累积奖励。公式为\n",
    "\n",
    "$$\n",
    "V^{\\pi}(s)=\\mathbb E_\\pi\\left[\\sum_{t=0}\\gamma^tR(s_t,a_t)|s_0=s\\right]\n",
    "$$\n",
    "\n",
    "- 在 RLHF 中的应用： 值函数 $V(x)$ 估计了在给定提示 $x$ 下，模型按照当前策略 $\\pi_\\theta$ 所能获得的预期累积奖励。\n",
    "-  在策略梯度方法中，策略的更新方向取决于优势函数\n",
    "\n",
    "    $$\n",
    "    A(y,x)=R(y)-V(x)\n",
    "    $$\n",
    "    - 优势函数衡量了特定动作（或回复） 𝑦 相对于状态 𝑥 下平均水平的优劣。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
