{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f3fee5-fd95-4008-a45a-6668b030d076",
   "metadata": {},
   "source": [
    "- https://pub.aimind.so/reinforcement-learning-meets-large-language-models-llms-aligning-human-preferences-in-llms-88c3a3f1a3f9\n",
    "- https://medium.com/@oleglatypov/a-comprehensive-guide-to-proximal-policy-optimization-ppo-in-ai-82edab5db200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4056257f-8947-4482-a948-b288375391f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T15:19:37.918352Z",
     "iopub.status.busy": "2024-11-11T15:19:37.917734Z",
     "iopub.status.idle": "2024-11-11T15:19:37.929219Z",
     "shell.execute_reply": "2024-11-11T15:19:37.927451Z",
     "shell.execute_reply.started": "2024-11-11T15:19:37.918305Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc0faba-9732-4ace-a305-329d61515fbd",
   "metadata": {},
   "source": [
    "- We define $r_\\theta(x, y)$ as the reward model being trained, parameterized by $\\theta$.\n",
    "- Where you can imagine that in the above equation, x is the prompt (state) given to the LLMs agent, and y is the completion (action) given by LLM:\n",
    "    - $S_t=x$\n",
    "    - $a_t=y$\n",
    "- LLM as the RL policy\n",
    "    - action: generate tokens\n",
    "    - a {prompt, completion} pair => rm => reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d608ff5b-c36f-49c4-97aa-fc1c5eb13b9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T15:20:44.304652Z",
     "iopub.status.busy": "2024-11-11T15:20:44.304019Z",
     "iopub.status.idle": "2024-11-11T15:20:44.316211Z",
     "shell.execute_reply": "2024-11-11T15:20:44.314132Z",
     "shell.execute_reply.started": "2024-11-11T15:20:44.304604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*WzmPShlUbdZwdBO65OtrMg.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://miro.medium.com/v2/resize:fit:4800/format:webp/1*WzmPShlUbdZwdBO65OtrMg.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9879fae2-da67-496a-b6a5-fbe64fbe0ca4",
   "metadata": {},
   "source": [
    "Having defined the objective function (above), we can use Proximal Policy Optimization (PPO) as a reinforcement learning algorithm. The main idea of PPO is to compute the gradient of the objective function with respect to the â€œpolicyâ€ parameter (Ï•), is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d5b89fe-b340-4ae4-b879-4b3d84eff8fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T15:21:09.894126Z",
     "iopub.status.busy": "2024-11-11T15:21:09.893538Z",
     "iopub.status.idle": "2024-11-11T15:21:09.905948Z",
     "shell.execute_reply": "2024-11-11T15:21:09.903896Z",
     "shell.execute_reply.started": "2024-11-11T15:21:09.894080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*vbnhEy2GFJPLfIP83LvuHQ.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://miro.medium.com/v2/resize:fit:640/format:webp/1*vbnhEy2GFJPLfIP83LvuHQ.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e3239-668e-4140-95ef-f8a28af8a962",
   "metadata": {},
   "source": [
    "### PPO with clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0459015d-c3b2-43fd-8a4a-3dcae3f1d987",
   "metadata": {},
   "source": [
    "- https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo\n",
    "- https://huggingface.co/learn/deep-rl-course/unit8/clipped-surrogate-objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0911ce7b-8f5c-4253-b2e3-43e5dee5b456",
   "metadata": {},
   "source": [
    "ä¸ºäº†ç¡®ä¿è®­ç»ƒçš„ç¨³å®šæ€§å¹¶é˜²æ­¢ç­–ç•¥å‘ç”Ÿè¿‡å¤§çš„æ›´æ–°ï¼Œå¯ä»¥ä½¿ç”¨**è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰**ç®—æ³•ã€‚PPO çš„ç›®æ ‡å‡½æ•°å¼•å…¥äº†ä¸€ä¸ª**æˆªæ–­çš„ä»£ç†æŸå¤±**ï¼š\n",
    "\n",
    "$$\n",
    "L^\\text{PPO}(\\theta) = \\mathbb{E}_{x \\sim D, y \\sim \\pi_\\theta} \\left[ \\min\\left( r_\\theta(y \\mid x) A(y, x), \\text{clip}\\left(r_\\theta(y \\mid x), 1 - \\epsilon, 1 + \\epsilon\\right) A(y, x) \\right) \\right]\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "- $ r_\\theta(y \\mid x) = \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{\\theta_{\\text{old}}}(y \\mid x)} $ æ˜¯**æ¦‚ç‡æ¯”å€¼**ã€‚\n",
    "- $ A(y, x) = R_\\phi(y) - V_{\\theta_{\\text{old}}}(x)$ æ˜¯**ä¼˜åŠ¿å‡½æ•°**ã€‚\n",
    "- $ \\epsilon $ æ˜¯æ§åˆ¶æˆªæ–­èŒƒå›´çš„è¶…å‚æ•°ã€‚\n",
    "- $ V_{\\theta_{\\text{old}}}(x) $ æ˜¯ä¼°è®¡ç»™å®š $ x $ çš„æœŸæœ›å¥–åŠ±çš„**å€¼å‡½æ•°**ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed7fff-84d9-4873-8b4f-fb19ef41ea63",
   "metadata": {},
   "source": [
    "#### å¥–åŠ±å‡½æ•°ä¸å€¼å‡½æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b33bfc-73a1-4c39-ad44-777b5a43ae7b",
   "metadata": {},
   "source": [
    "- å®šä¹‰ï¼š å¥–åŠ±å‡½æ•° $R(s,a)$ å®šä¹‰äº†åœ¨çŠ¶æ€ $s$ ä¸‹æ‰§è¡ŒåŠ¨ä½œ $a$ åè·å¾—çš„å³æ—¶å¥–åŠ±ã€‚\n",
    "    - åœ¨ RLHF ä¸­çš„åº”ç”¨ï¼š åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸­ï¼Œå¥–åŠ±å‡½æ•°é€šå¸¸è¡¨ç¤ºä¸º $R(y)$ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹ç”Ÿæˆçš„å›å¤ $y$ çš„è´¨é‡ã€‚è¿™é‡Œï¼Œå¥–åŠ±æ¨¡å‹ $R_\\phi(y)$ å·²é€šè¿‡äººç±»åå¥½æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿä¸ºæ¯ä¸ªå›å¤ $y$ æä¾›ä¸€ä¸ªæ ‡é‡å¥–åŠ±ã€‚\n",
    "- å€¼å‡½æ•° $V(s)$ è¡¨ç¤ºåœ¨çŠ¶æ€ $s$ ä¸‹ï¼Œéµå¾ªç­–ç•¥ $\\pi$ æ‰€èƒ½è·å¾—çš„é¢„æœŸç´¯ç§¯å¥–åŠ±ã€‚å…¬å¼ä¸º\n",
    "\n",
    "$$\n",
    "V^{\\pi}(s)=\\mathbb E_\\pi\\left[\\sum_{t=0}\\gamma^tR(s_t,a_t)|s_0=s\\right]\n",
    "$$\n",
    "\n",
    "- åœ¨ RLHF ä¸­çš„åº”ç”¨ï¼š å€¼å‡½æ•° $V(x)$ ä¼°è®¡äº†åœ¨ç»™å®šæç¤º $x$ ä¸‹ï¼Œæ¨¡å‹æŒ‰ç…§å½“å‰ç­–ç•¥ $\\pi_\\theta$ æ‰€èƒ½è·å¾—çš„é¢„æœŸç´¯ç§¯å¥–åŠ±ã€‚\n",
    "-  åœ¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸­ï¼Œç­–ç•¥çš„æ›´æ–°æ–¹å‘å–å†³äºä¼˜åŠ¿å‡½æ•°\n",
    "\n",
    "    $$\n",
    "    A(y,x)=R(y)-V(x)\n",
    "    $$\n",
    "    - ä¼˜åŠ¿å‡½æ•°è¡¡é‡äº†ç‰¹å®šåŠ¨ä½œï¼ˆæˆ–å›å¤ï¼‰ ğ‘¦ ç›¸å¯¹äºçŠ¶æ€ ğ‘¥ ä¸‹å¹³å‡æ°´å¹³çš„ä¼˜åŠ£ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
