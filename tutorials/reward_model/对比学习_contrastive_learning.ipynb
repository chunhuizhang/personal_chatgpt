{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65cb11f2",
   "metadata": {},
   "source": [
    "- https://lilianweng.github.io/posts/2021-05-31-contrastive/\n",
    "- siamese-triplet\n",
    "    - https://github.com/adambielski/siamese-triplet/tree/master\n",
    "- contrastive-image-text\n",
    "    - CLIP：https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0083f42-32c4-4466-887c-7a4f51729825",
   "metadata": {},
   "source": [
    "### constrastive learning & metric learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c955783d-6ac2-4f13-b967-8e609897ad95",
   "metadata": {},
   "source": [
    "- constrastive learning\n",
    "    - 一种通过对样本对进行比较来学习有用表征的方法。目标是使相似样本的表示更接近，不相似样本的表示更远离。\n",
    "    - 相似不相似，更多是类别上的；\n",
    "- metric learning or embedding learning\n",
    "    - embedding learning：学习的是表示；\n",
    "        - 传统对比学习主要关注学习数据的表示（embedding），使得相似样本的表示更接近，以便在下游任务中使用。\n",
    "- learning to rank（learning from preference）\n",
    "    - 从类别到偏序\n",
    "- 数据集（样本对，正例、负例，或者偏序对）的构造，以及损失函数的设计；\n",
    "- contrastive learning vs. reward modeling\n",
    "    - contrastive learning 学到的是一个表示（representation），或者说基于表示还可进一步算距离 $\\|f(x)-f(x_+)\\|$；\n",
    "    - Reward modelling 你最终学到的是一个 reward/value function：$r_\\theta(x,y_w)$；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c9121-a94f-426e-99a4-46bdc7cfae57",
   "metadata": {},
   "source": [
    "### CLIP (Contrastive Language–Image Pretraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a95e624-1fe6-48a9-a516-53af5986feb6",
   "metadata": {},
   "source": [
    "> 相应的图像和文本在特征空间中接近，而不相关的图文对则远离\n",
    "\n",
    "- 在训练过程中，CLIP 处理一批（batch）图文对。例如，在一个批次中有 N 个图像和 N 个对应的文本描述。模型将每个图像和文本分别编码为向量表示。然后，它计算每个图像向量和每个文本向量之间的相似度，形成一个 N x N 的相似度矩阵。\n",
    "    - 对于这个矩阵中的每一行，模型希望对应的正确匹配（即第 i 个图像和第 i 个文本）的相似度最大，而其他不匹配的相似度较小。为此，CLIP 使用了一种基于交叉熵的对比损失函数，它考虑了图像到文本和文本到图像的匹配，这意味着模型同时优化图像检索文本和文本检索图像的任务。\n",
    "    - 这种损失函数的设计使得模型学会在高维特征空间中，将相关的图像和文本映射到相近的位置，从而实现跨模态的检索和匹配。\n",
    "- 设在一个批次中，有 N 个图像和 N 个对应的文本描述，形成 N 个匹配的图文对。\n",
    "    - 令 $I_i$ 表示第 $i$ 个图像，$T_i$ 表示第 $i$ 个文本\n",
    "    - 定义图像编码器 $f$ 和文本编码器 $g$，将图像和文本映射到一个共同的特征空间。（表征学习 representation learning）\n",
    "    - 获取归一化的图像和文本特征向量：\n",
    "\n",
    "        $$\n",
    "        \\mathbf v_i=\\frac{f(I_i)}{\\|f(I_t)\\|}, \\mathbf u_i=\\frac{g(T_i)}{\\|g(T_i)\\|}\n",
    "        $$\n",
    "    - 相似度矩阵 $S$ 的元素定义为（其中，$\\tau$ 是可学习的温度（temperature）参数，用于控制分布的锐化程度。）\n",
    "        - 注意 $v_i$ 和 $u_i$ 维度未必一致，也许还需要一层映射，映射到一个共同的表征空间；\n",
    "        $$\n",
    "        s_{ij}=\\frac{v_i^Tu_i}{\\tau}\n",
    "        $$\n",
    "    - 对于图像到文本的方向，定义预测的概率分布：\n",
    "        $$\n",
    "        p_{ij}=\\frac{\\exp(s_{ij})}{\\sum_{k=1}^N\\exp(s_{ik})}\n",
    "        $$\n",
    "        - 相应的损失函数为：$\\mathcal L_{\\text{img2txt}}=-\\frac1N\\sum_{i=1}^N\\log p_{i,i}$\n",
    "    - 对于文本到图像的方向，定义预测的概率分布为：\n",
    "        $$\n",
    "        q_{ij}=\\frac{\\exp(s_{ij})}{\\sum_{k=1}^N\\exp(s_{k,j})}\n",
    "        $$\n",
    "        - 相应的损失函数为：$\\mathcal L_{\\text{txt2img}}=-\\frac1N\\sum_{i=1}^N\\log q_{i,i}$\n",
    "    - 总的损失\n",
    "        $$\n",
    "        \\mathcal L=\\frac12({L_{\\text{img2txt}}+L_{\\text{txt2img}}})\n",
    "        $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908edeb4-8070-4851-a935-affc9f6682ec",
   "metadata": {},
   "source": [
    "### reward modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962b4a5d-9530-439f-9c0d-8ed5cc2c5aea",
   "metadata": {},
   "source": [
    "- 而 instruct GPT 开创的 reward modeling 追求的是学习 reward function，即给定一个 input, response pair，输出一个好坏的分数（scalar output）\n",
    "    $$\n",
    "    \\text{loss}(\\theta) = -\\frac{1}{\\left(\\frac{K}{2}\\right)} \\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\log \\left( \\sigma \\left( r_\\theta(x, y_w) - r_\\theta(x, y_l) \\right) \\right) \\right]\n",
    "    $$ \n",
    "- 对比学习，强调的是 $f(I_i),g(T_i)$ 将原始的输入，映射到 hidden space，得到的 feature，即为表征学习（representation learning）；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8888c5c1",
   "metadata": {},
   "source": [
    "## constrastive loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14215a98",
   "metadata": {},
   "source": [
    "### (pairwise) Margin Ranking loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a491c4",
   "metadata": {},
   "source": [
    "$$\n",
    "L(y_w,y_l)=\\max(0, \\delta-(R(y_w) - R(y_l))\n",
    "$$\n",
    "\n",
    "- 也就是说，目标追求的是 $R(y_w)-R(y_l)\\geq \\delta$ 时，loss 才为0；\n",
    "\n",
    "    $$\n",
    "    R(y_w)-R(y_l)\\geq \\delta\\\\\n",
    "    R(y_w)\\geq R(y_l)+\\delta\n",
    "    $$\n",
    "    \n",
    "    - Reward model 对 $y_w$ 的打分要比对 $y_l$ 的打分 + $\\delta$ 还多；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
