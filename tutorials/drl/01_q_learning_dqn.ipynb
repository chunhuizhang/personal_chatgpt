{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9f05039",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T14:59:50.597354Z",
     "start_time": "2023-11-14T14:59:50.589571Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70929104",
   "metadata": {},
   "source": [
    "## basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c3eb3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "U_t&=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2} + \\cdots + \\gamma^{n-t} R_n\\\\\n",
    "&=\\sum_{k=t}^n\\gamma^{k-t}R_k{}\\\\\n",
    "Q_\\pi(s_t,a_t)&=\\mathbb E\\left[U_t|S_t=s_t,A_t=a_t\\right]\\\\\n",
    "&=\\mathbb E_{S_{t+1},A_{t+1},\\cdots, S_n,A_n}\\left[U_t|S_t=s_t,A_t=a_t\\right]\\\\\n",
    "Q_\\star(s_t,a_t)&=\\max_\\pi Q_\\pi(s_t,a_t)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- $U_t$: discounted return,\n",
    "    - return: cumulative future reward\n",
    "- $Q_\\pi(s_t,a_t)$：action value function，动作价值函数\n",
    "    - 其计算式中的期望消除了 $t$ 时刻之后的所有状态 $S_{t+1},\\cdots, S_n$ 与所有动作 $A_{t+1}, \\cdots, A_n$ 的随机性（也是 $U_t$ 随机性的来源）\n",
    "        - 未来时刻的 $S_{t+1}, A_{t+1}$ 会带来 $R_{t+1}$\n",
    "    - 对谁求期望就是消除对谁的随机性；\n",
    "- $Q_\\star(s_t,a_t)$：最优动作价值函数；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f2b7aa",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b8d76",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(s,a;w)\n",
    "$$\n",
    "\n",
    "- $w$ 表示 NN 的模型参数；\n",
    "- DQN 的输入是 $s$，输出是离散动作空间 $\\mathcal A$ 中的每个动作的 Q 值，是一个 scalar value；\n",
    "- train DQN 的目标即是，对所有的 $s,a$ pair， DQN 对 $Q(s,a;w)$ 的预测尽量接近 $Q_\\star(s,a)$\n",
    "- 训练 DQN 最常用的算法是 TD（temporal difference，时间差分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22d5fb1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T12:23:21.170351Z",
     "start_time": "2023-11-14T12:23:21.163694Z"
    }
   },
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, dim_state, num_action):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim_state, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_action)\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ae38a",
   "metadata": {},
   "source": [
    "## TD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a001b591",
   "metadata": {},
   "source": [
    "- TD target 与 TD error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e65d9",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "U_t&=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2} + \\cdots + \\gamma^{n-t} R_n\\\\\n",
    "&=\\sum_{k=t}^n\\gamma^{k-t}R_k\\\\\n",
    "&=R_{t}+\\sum_{k=t+1}^n\\gamma^{k-t}R_{k}\\\\\n",
    "&=R_{t}+\\gamma\\sum_{k=t+1}^n\\gamma^{k-(t+1)}R_{k}\\\\\n",
    "&=R_{t}+\\gamma U_{t+1}\\\\\n",
    "Q_\\star(s_t,a_t)&=\\max_\\pi\\mathbb E\\left[U_t|S_t=s_t,A_t=a_t\\right]\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- $U_t$ 的递归定义与计算\n",
    "- 基于上述的两个等式可以推导出著名的最优贝尔曼方程（optimal Bellman equation）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180802c3",
   "metadata": {},
   "source": [
    "### optimal Bellman equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a9ac6",
   "metadata": {},
   "source": [
    "$$\n",
    "Q_\\star(s_t,a_t)=\\mathbb E_{S_{t+1}\\sim p(\\cdot|s_t,a_t)}\\left[R_t+\\gamma\\cdot\\max_{\\pi}Q_\\star(S_{t+1}, A)\\bigg|S_t=s_t,A_t=a_t\\right]\n",
    "$$\n",
    "\n",
    "- 也是一种 recursive 的计算方式\n",
    "- 右侧是一个期望，期望可以通过蒙特卡洛方法近似，当 Agent 在状态 $s_t$ 执行动作 $a_t$ 之后，环境通过状态转移函数 $p(s_{t+1}|s_t,a_t)$（mdp）计算出下一时刻的状态 $s_{t+1}$，因此当我们观测到 $s_t,a_t,s_{t+1}$ 时，奖励 $R_t$ 也会被观测到，记作 $r_t$，于是有了如下的四元组\n",
    "\n",
    "$$\n",
    "(s_t,a_t,r_t,s_{t+1})\n",
    "$$\n",
    "\n",
    "- 基于蒙特卡洛估计，可以算出等式右边期望的近似\n",
    "\n",
    "$$\n",
    "Q_\\star(s_t,a_t) \\approx r_t+\\gamma\\cdot \\max_{a\\in\\mathcal A}Q_\\star(s_{t+1},a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3b1b8",
   "metadata": {},
   "source": [
    "### 基于 TD train DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5268db55",
   "metadata": {},
   "source": [
    "- 将最优动作价值函数 $Q_\\star(s_t,a_t)$ 替换为神经网络 $Q(s,a;w)$\n",
    "\n",
    "$$\n",
    "\\underbrace{Q(s_t,a_t;w)}_{预测 \\hat{q}_t}\\approx \\underbrace{r_t + \\gamma\\cdot \\max_{a\\in\\mathcal A}Q(s_{t+1},a;w)}_{\\text{TD target}, \\hat y_t}\n",
    "$$\n",
    "\n",
    "- 左边的 $Q(s_t,a_t;w)$ 是神经网络在 $t$ 时刻做出的预测：$\\hat q_t$\n",
    "- 右边的 TD target 则是神经网络在 $t+1$ 时刻做出的预测：$\\hat y_t$，且基于了真实观测到的奖励 $r_t$（多了一部分事实）\n",
    "- 对于右式 $\\hat y_t$ 比着左式 $\\hat q_t$ 多了部分事实，因此更可信，**在监督学习的范式下**，可以作为训练的 groud truth（将其视为常数），定义如下的损失\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "L(w)&=\\frac12\\left[Q(s_t,a_t;w)-\\hat y_t\\right]^2=\\frac12\\left[\\hat q_t-\\hat y_t\\right]^2\\\\\n",
    "\\nabla_wL(w)&=\\underbrace{(\\hat q_t-\\hat y_t)}_{\\text{TD error}}\\cdot \\nabla_wQ(s_t,a_t;w)=\\delta_t\\cdot \\nabla_wQ(s_t,a_t;w)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- 梯度下降优化让 $\\hat q_t$ 更接近 $\\hat y_t$\n",
    "\n",
    "$$\n",
    "w\\leftarrow w-\\alpha\\cdot\\delta_t\\nabla_wQ(s_t,a_t;w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ceeab",
   "metadata": {},
   "source": [
    "### training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c72da",
   "metadata": {},
   "source": [
    "- 给定一个四元组 $(s_t,a_t,r_t,s_{t+1})$ 一个 DQN 网络，我们可以得到\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\hat q_t&=Q(s_t,a_t;w)\\\\\n",
    "\\hat y_t&=r_t+\\max_{a\\in\\mathcal A}Q(s_{t+1},a;w)\\\\\n",
    "\\delta_t&=\\hat q_t-\\hat y_t\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- 梯度下降优化让 $\\hat q_t$ 更接近 $\\hat y_t$\n",
    "\n",
    "$$\n",
    "w\\leftarrow w-\\alpha\\cdot\\delta_t\\nabla_wQ(s_t,a_t;w)\n",
    "$$\n",
    "\n",
    "\n",
    "- 训练 DQN 所需的四元组数据 $(s_t,a_t,r_t,s_{t+1})$ 与控制 agent 运动的策略无关，意味着可以用任何策略控制智能体与环境交互，同时记录下算法运动轨迹（trajectory），作为 DQN 的训练数据；\n",
    "- 因此 DQN 的训练可以分为两个独立的部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b300f",
   "metadata": {},
   "source": [
    "#### 收集训练数据    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b5419",
   "metadata": {},
   "source": [
    "- 可以用任意策略 $\\pi$ 来控制智能体与环境交互，此时的 $\\pi$ 可以称为行为策略，一种经典的行为策略是 $\\epsilon$-greedy 策略\n",
    "        \n",
    "$$\n",
    "a_t=\\begin{cases}\n",
    "\\arg\\max_a Q(s_t,a;w), & r < 1-\\epsilon (以概率 (1-\\epsilon), )\\\\\n",
    "均匀采样 a\\in \\mathcal A, & r < \\epsilon  (以概率 \\epsilon)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- 将 Agent 在一个 episode （回合） 中的轨迹记作\n",
    "\n",
    "$$\n",
    "(s_1,a_1,r_1), (s_2,a_2,r_2), (s_3,a_3,r_3), \\cdots, (s_n,a_n,r_n)\n",
    "$$\n",
    "\n",
    "- 进一步将其划分为 $(s_t,a_t,r_t,s_{t+1})$ 这样的四元组，存入缓存，这个缓存就叫经验回放缓存（Experience Replay Buffer）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a3e56c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T12:28:30.179692Z",
     "start_time": "2023-11-14T12:28:30.154350Z"
    }
   },
   "outputs": [],
   "source": [
    "# 循环队列\n",
    "# 两个核心函数：\n",
    "# 1. push 队列中循环添加\n",
    "# 2. sample：队列中采样\n",
    "\n",
    "@dataclass\n",
    "class ReplayBuffer:\n",
    "    maxszie: int\n",
    "    size: int = 0\n",
    "        \n",
    "    # s_t\n",
    "    states: list = field(default_factory=list)\n",
    "    # a_t\n",
    "    actions: list = field(default_factory=list)\n",
    "    # r_t\n",
    "    rewards: list = field(default_factory=list)\n",
    "    # s_{t+1}\n",
    "    next_states: list = field(default_factory=list)\n",
    "    \n",
    "    dones: list = field(default_factory=list)\n",
    "    \n",
    "    def push(self, state, action, reward, done, next_state):\n",
    "        if self.size < self.maxsize:\n",
    "            # state.shape: \n",
    "            self.states.append(state)\n",
    "            # action.shape: \n",
    "            self.actions.append(action)\n",
    "            # reward.shape: \n",
    "            self.rewards.append(reward)\n",
    "            # done.shape: \n",
    "            self.dones.append(done)\n",
    "            # next_state.shape: \n",
    "            self.next_states.append(next_state)\n",
    "        else:\n",
    "            # overlap\n",
    "            index = self.size % self.maxsize\n",
    "            self.states[index] = state\n",
    "            self.actions[index] = action\n",
    "            self.rewards[index] = reward\n",
    "            self.dones[index] = done\n",
    "            self.next_states[index] = next_state\n",
    "        self.size += 1\n",
    "    \n",
    "    def sample(self, n):\n",
    "        total_number = min(self.size, self.maxsize)\n",
    "        indices = np.random.randint(total_number, size=n)\n",
    "        sample_states = [self.states[ind] for ind in indices]\n",
    "        sample_actions = [self.actions[ind] for ind in indices]\n",
    "        sample_rewards = [self.rewards[ind] for ind in indices]\n",
    "        sample_dones = [self.dones[ind] for ind in indices]\n",
    "        sample_next_states = [self.next_states[ind] for ind in indices]\n",
    "        return sample_states, sample_actions, sample_rewards, sample_dones, sample_next_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f93ce1",
   "metadata": {},
   "source": [
    "####  `forward/backward` 监督训练过程\n",
    "\n",
    "> 从 Experience Replay Buffer 中随机选出一个四元组，记作 $(s_j,a_j,r_j,s_{j+1})$，假定 DQN 当前的参数为 $w_{now}$，经过如下的 forward/backward 步骤对参数更新，得到 $w_{new}$\n",
    "\n",
    "\n",
    "1. forward：分别输入 $s_j$ 和 $s_{j+1}$（两个都是标量值）\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\hat q_j&=Q(s_j,a_j;w_{now})\\\\\n",
    "\\hat q_{j+1}&=\\max_{a\\in\\mathcal A}Q(s_{j+1},a;w_{now})\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "2. 计算 TD target 与 TD error，以及定义 loss\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\hat y_j&=r_j+\\gamma \\hat q_{j+1}, \\\\\n",
    "\\delta_j&=\\hat q_j-\\hat y_j\\\\\n",
    "L(w)&=\\frac12 \\delta_j^2=\\frac12\\left(\\hat q_j-\\hat y_j\\right)^2\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "3. loss 反向传播，计算梯度（`loss.backward()` 自动计算）\n",
    "\n",
    "$$\n",
    "g_j=\\nabla_wQ(s_j,a_j;w_{now})\n",
    "$$\n",
    "\n",
    "4. `optimizer.step()` 基于梯度下降更新参数\n",
    "\n",
    "$$\n",
    "w_{new}\\Leftarrow w_{old}-\\alpha\\cdot\\delta_j\\cdot g_j\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1edae20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T12:23:34.302652Z",
     "start_time": "2023-11-14T12:23:34.273217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7624,  0.6647,  0.7374, -0.3199,  1.8371])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "print(x)\n",
    "x.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a45a7e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T12:43:02.566294Z",
     "start_time": "2023-11-14T12:43:02.541875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1709, 0.0343],\n",
       "        [0.1366, 0.0705],\n",
       "        [0.2027, 0.0694],\n",
       "        [0.1014, 0.2124],\n",
       "        [0.2879, 0.0034]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QNet(4, 2)(torch.randn(5, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a49b9b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T13:29:48.400893Z",
     "start_time": "2023-11-14T13:29:48.387114Z"
    }
   },
   "outputs": [],
   "source": [
    "# 实例化之后，可以视为 agent\n",
    "class DQN:\n",
    "    def __init__(self, dim_state, num_action, gamma=0.9):\n",
    "        self.gamma = gamma\n",
    "        # 1. 稳定学习过程，主要靠 self.target_Q 较慢的更新\n",
    "        # 2. 减少自相关性；\n",
    "        # self.Q：当前值，也是要更新的主网络；\n",
    "        self.Q = QNet(dim_state, num_action)\n",
    "        # self.target_Q: 未来值\n",
    "        self.target_Q = QNet(dim_state, num_action)\n",
    "        self.target_Q.load_state_dict(self.Q.state_dict())\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        q_vals = self.Q(state)\n",
    "        return q_vals.argmax()\n",
    "    \n",
    "    def comp_loss(self, batch_s, batch_a, batch_r, batch_d, batch_next_s):\n",
    "        # https://www.bilibili.com/video/BV1YY4y1Q72M/\n",
    "        q_vals = self.Q(batch_s).gather(1, batch_a.unsqueeze(1)).squeeze()\n",
    "        # Returns a new Tensor, detached from the current graph.\n",
    "        # The result will never require gradient.\n",
    "        next_q_vals = self.target_Q(batch_next_s).detach().max(dim=1)\n",
    "        loss = F.mse_loss(batch_r + self.gamma * next_q_vals * (1-batch_d), q_vals)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e30229",
   "metadata": {},
   "source": [
    "## train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb0fee85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T14:58:50.537581Z",
     "start_time": "2023-11-14T14:58:50.528480Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (384133942.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[35], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def train(env, agent: DQN, lr=1e-3, max_steps=100_1000):\n",
    "    replay_buffer = ReplayBuffer(10_1000)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(agent.Q.parameters(), lr=lr)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    epsilon = 1\n",
    "    \n",
    "    agent.Q.train()\n",
    "    state = env.reset()\n",
    "    for step in range(max_steps):\n",
    "        if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c1f78ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T13:31:19.054918Z",
     "start_time": "2023-11-14T13:31:19.043598Z"
    }
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7443a269",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T13:33:11.506456Z",
     "start_time": "2023-11-14T13:33:11.498314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06f93727",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T13:35:36.563734Z",
     "start_time": "2023-11-14T13:35:36.556614Z"
    }
   },
   "outputs": [],
   "source": [
    "dim_state = env.observation_space.shape[0]\n",
    "num_action = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8280cf2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T14:49:50.867775Z",
     "start_time": "2023-11-14T14:49:50.861947Z"
    }
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c18718e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T14:51:29.105738Z",
     "start_time": "2023-11-14T14:51:27.442510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNet(\n",
       "  (fc1): Linear(in_features=4, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "agent = DQN(dim_state, num_action, gamma)\n",
    "agent.Q.to(device)\n",
    "agent.target_Q.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972420f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "218px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
