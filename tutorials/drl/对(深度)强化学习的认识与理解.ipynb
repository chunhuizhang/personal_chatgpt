{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2a9d31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T06:46:36.028221Z",
     "start_time": "2023-12-03T06:46:36.014065Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acedc5be",
   "metadata": {},
   "source": [
    "## 基本设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c742145e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T06:47:18.085579Z",
     "start_time": "2023-12-03T06:47:18.079108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../../imgs/agent_rl.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../../imgs/agent_rl.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2f4bf",
   "metadata": {},
   "source": [
    "- reward 从哪里来\n",
    "    - 目前都是 agent 跟 env 交互的过程中，从 env 中来；\n",
    "    - 但现实世界中的 human，其 reward 是自我奖励的；\n",
    "        - 最本质\n",
    "- agent = neural network\n",
    "    - 尤其是在 DRL 中，如上图所示；\n",
    "    - 也就是 nn map observations to actions；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21651e",
   "metadata": {},
   "source": [
    "## model base vs. model free"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ab1bcc",
   "metadata": {},
   "source": [
    "\n",
    "- model free: 这些方法不使用环境的模型。它们通过直接与环境交互（interaction：$s_t,a_t,r_t$）来学习价值函数或策略，而不尝试预测环境的动态。\n",
    "    - policy gradient\n",
    "    - Q learning \n",
    "- model base：使用或学习一个环境的模型。\n",
    "    - 这个模型可以是显式的，比如一个状态转移矩阵 ($P(s'|s,a)$) 和奖励函数 ($R(s,a,s')$)，或是隐式的，比如一个可以预测下一个状态和奖励的神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb679af5",
   "metadata": {},
   "source": [
    "## value based vs. policy based method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8040f0",
   "metadata": {},
   "source": [
    "- 都是为了得到 action policy（求解序列决策问题）\n",
    "    - value based method：间接地通过 $V(s)/Q(s,a)$ 来选择动作；\n",
    "    - policy based method：直接通过 $\\pi(s)$ 选择动作；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
