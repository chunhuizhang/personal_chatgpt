{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6be0576c-ee48-4141-a38f-960562a2ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import torch\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594095e0-c827-423b-97cf-61b1cdd083a7",
   "metadata": {},
   "source": [
    "## why position encoding in Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59af1a3-244e-43c0-bf30-b5a869eeff6a",
   "metadata": {},
   "source": [
    "- attention mechanism （Transformer 最特色的）\n",
    "    - $X\\in\\mathbb R^{\\ell\\times d}$\n",
    "    - $W_k\\in\\mathbb R^{d\\times d_k},W_q\\in\\mathbb R^{d\\times d_k},W_v\\in\\mathbb R^{d\\times d_v}$\n",
    "    - $Q=XW_q\\in\\mathbb R^{\\ell\\times d_k}, K=XW_k\\in\\mathbb R^{\\ell\\times d_k}, V=XW_v\\in\\mathbb R^{\\ell\\times d_v}$\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "$$\n",
    "A_{ij}=\\frac{\\exp(\\frac{q^T_ik_j}{\\sqrt{d_k}})}{\\sum_{j'}\\exp(\\frac{q^T_ik_{j'}}{\\sqrt{d_k}})}\n",
    "$$\n",
    "\n",
    "- $A_{ij}$ （attention weights， $QK^T$: attention scores） 表示的是位置 $i$ 的词（token）与位置 $j$ 的词（token）的注意力权重，\n",
    "    - 就是如果 $x_i$/$x_j$ 或者 $q_i$/$q_j$($k_i$/$k_j$) 没有编码位置信息，那么的话，这个weight就跟位置无关，显然在 seq modeling 中是有很大缺陷的\n",
    "    - 也就是一句话的含义，肯定跟 token 的组织有序有关"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21004564-68c6-4dd1-8d48-e128f479ff60",
   "metadata": {},
   "source": [
    "## bert, gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa03d15f-3f62-49d3-acb6-aaea0b505f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:786/format:webp/1*iJqlhZz-g6ZQJ53-rE9VvA.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://miro.medium.com/v2/resize:fit:786/format:webp/1*iJqlhZz-g6ZQJ53-rE9VvA.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f573fb-6c0b-4754-8529-976ab8be7948",
   "metadata": {},
   "source": [
    "- BERT: 加性 (absolue) position encoding （learnable position encoding）\n",
    "\n",
    "    ```\n",
    "    # modeling_bert.py\n",
    "    embeddings = inputs_embeds + token_type_embeddings + self.position_embeddings(position_ids)\n",
    "    ```\n",
    "\n",
    "- GPT: 加性 （absolute）position encoding（learnable position encoding）\n",
    "\n",
    "    ```\n",
    "    # modeling_gpt.py\n",
    "    if inputs_embeds is None:\n",
    "        inputs_embeds = self.wte(input_ids)\n",
    "    position_embeds = self.wpe(position_ids)\n",
    "    hidden_states = inputs_embeds + position_embeds + token_type_embeds\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1fee0d-5314-4eaa-8ddb-a2c34d0fb6d6",
   "metadata": {},
   "source": [
    "### sin position encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd388ab1-f3d3-442f-8aa0-3097b12b49b3",
   "metadata": {},
   "source": [
    "- 无需训练; 依然是绝对位置编码\n",
    "- transformers 原始论文\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "PE(t,2i)&=\\sin(\\frac{t}{10000^{\\frac{2i}{d_{model}}}})\\\\\n",
    "PE(t,2i+1)&=\\cos(\\frac{t}{10000^{\\frac{2i}{d_{model}}}})\\\\\n",
    "\\Downarrow\\\\\n",
    "PE(t,i)&=\\sin(\\frac{t}{10000^{\\frac{i}{d_{model}}}}), \\quad \\text{i is even}\\\\\n",
    "PE(t,i)&=\\cos(\\frac{t}{10000^{\\frac{i-1}{d_{model}}}}), \\quad \\text{i is odd}\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "d_model = 4\n",
    "\n",
    "- pos = 0, $[\\sin(0),\\cos(0), \\sin(0),\\cos(0)]$\n",
    "- pos = 1, $[\\sin\\left(\\frac{1}{10000^{0/4}}\\right),\\cos\\left(\\frac{1}{10000^{0/4}}\\right), \\sin\\left(\\frac{1}{10000^{2/4}}\\right), \\cos\\left(\\frac{1}{10000^{2/4}}\\right)]$\n",
    "- pos = 2, $[\\sin\\left(\\frac{2}{10000^{0/4}}\\right),\\cos\\left(\\frac{2}{10000^{0/4}}\\right), \\sin\\left(\\frac{2}{10000^{2/4}}\\right), \\cos\\left(\\frac{2}{10000^{2/4}}\\right)]$\n",
    "- pos = 3, $[\\sin\\left(\\frac{3}{10000^{0/4}}\\right),\\cos\\left(\\frac{3}{10000^{0/4}}\\right), \\sin\\left(\\frac{3}{10000^{2/4}}\\right), \\cos\\left(\\frac{3}{10000^{2/4}}\\right)]$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf1cb7-d7d8-49d1-a42a-063816cd5b75",
   "metadata": {},
   "source": [
    "## llama RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44949d8-0a3b-4be1-861d-ebbb8775247f",
   "metadata": {},
   "source": [
    "- 从绝对位置编码到相对位置编码\n",
    "    - 绝对位置编码，位置 pos_i 的编码仅取决于 pos_i 的值；\n",
    "    - 相对位置编码，（一般不需要对每个位置进行单独的编码），而是直接对位置之间的相对距离进行编码\n",
    "        - pos=0 与 pos=1 的相对位置 $f(|0-1|)$\n",
    "        - pos=1 与 pos=3 的相对位置 $f(|1-3|)$\n",
    "        - 偏差构成的矩阵，称为 id 矩阵；\n",
    "- RoPE\n",
    "    - 旋转位置编码，为相对位置编码，非加性位置编码，直接嵌入到 attention mechanism 的计算中；\n",
    "    - $R^d_{\\Theta,m}$：位置 $m$ 对应的旋转矩阵 not learnable：非学习的，全局固定的；\n",
    "        - $m\\theta$：frequency\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "f(q,m)^Tf(k,n)&=(R_mq)^T(R_nk)\\\\\n",
    "&=q^T(R^T_mR_n)k\\\\\n",
    "&=q^TR_{n-m}k\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "\n",
    "```\n",
    "# freqs_cis 是一个全局的旋转矩阵\n",
    "xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "xq, xk, xv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ea644ce-892d-45d7-a91a-cc48a773377f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../../imgs/rope_paper.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d: dim, \n",
    "# m: position\n",
    "Image(url='../../imgs/rope_paper.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b60dba04-3734-4117-afff-e79e483c3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c45b44a-1686-46fc-8411-dd3564a083aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000+0.0000j,  1.0000+0.0000j],\n",
       "        [ 0.5403+0.8415j,  0.9999+0.0100j],\n",
       "        [-0.4161+0.9093j,  0.9998+0.0200j]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cis = precompute_freqs_cis(dim=4, end=3)\n",
    "cis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865d8eb-034a-4677-b82b-6ea5f6dc80af",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "&\\text{freqs}=[1,\\frac{1}{\\theta^{2/4}}]=[1., 0.01]\\\\\n",
    "&t=[0,1,2]\\\\\n",
    "&\\text{freqs}=\\begin{bmatrix}\n",
    "0 & 0\\\\\n",
    "1 & 0.01\\\\\n",
    "2 & 0.02\n",
    "\\end{bmatrix}\\\\\n",
    "&\\text{freqs\\_cis}=e^{j\\cdot\\text{freqs}}=\\begin{bmatrix}\n",
    "1 & 1\\\\\n",
    "e^j & e^{j\\cdot0.01}\\\\\n",
    "e^{j\\cdot2} & e^{j\\cdot0.02}\n",
    "\\end{bmatrix}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7abede1c-7f2f-4cd4-be8e-9ed1902dac9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4161+0.9093j])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.polar(torch.tensor([1.]), torch.tensor([0.]))\n",
    "torch.polar(torch.tensor([1.]), torch.tensor([2.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2088bd5-df13-42e9-82ea-8c0a3bdd1cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000+0.0000j,  1.0000+0.0000j],\n",
       "        [ 0.5403+0.8415j,  0.9999+0.0100j],\n",
       "        [-0.4161+0.9093j,  0.9998+0.0200j]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_matrix = torch.tensor([\n",
    "    [0.0, 0.0],\n",
    "    [1.0, 0.01],\n",
    "    [2.0, 0.02]\n",
    "])\n",
    "\n",
    "# 幅度矩阵，全为 1\n",
    "r_matrix = torch.ones_like(theta_matrix)\n",
    "\n",
    "# 计算 e^{j*theta_matrix}\n",
    "e_j_theta_matrix = torch.polar(r_matrix, theta_matrix)\n",
    "e_j_theta_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a5a499-66bd-4783-af32-5e32a848e061",
   "metadata": {},
   "source": [
    "### https://spaces.ac.cn/archives/8265"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee09035c-15df-446f-95a9-92a4852803bc",
   "metadata": {},
   "source": [
    "- 二维向量的旋转矩阵是 $2\\times 2$的正交矩阵\n",
    "- d维向量的旋转矩阵是 d*d，依然是正交矩阵\n",
    "    - 注意 $x_1,x_2$ 对应一个2*2旋转矩阵 $R_{1,2}$\n",
    "    - $x_3,x_4$ 对应一个2*2的旋转矩阵 $R_{3,4}$\n",
    "    - ...\n",
    "- $R=R_{1,2}R_{3,4}R_{5,6}\\cdots R_{d-1,d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6f87839-acab-474c-8628-c0876383d4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../../imgs/4d_rotation.png\" width=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../../imgs/4d_rotation.png', width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b574c91-93cb-421c-8123-34800db462b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../../imgs/rope_1.png\" width=\"450\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../../imgs/rope_1.png', width=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d87f4c7-086f-487e-8b86-36f644866164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.8660, -0.5000,  0.0000,  0.0000],\n",
       "         [ 0.5000,  0.8660,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  1.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  1.0000]]),\n",
       " tensor([[ 1.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  1.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.5000, -0.8660],\n",
       "         [ 0.0000,  0.0000,  0.8660,  0.5000]]),\n",
       " tensor([[ 0.8660, -0.5000,  0.0000,  0.0000],\n",
       "         [ 0.5000,  0.8660,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.5000, -0.8660],\n",
       "         [ 0.0000,  0.0000,  0.8660,  0.5000]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta1 = torch.tensor(np.pi/6)\n",
    "theta2 = torch.tensor(np.pi/3)\n",
    "\n",
    "R12 = torch.tensor([\n",
    "    [torch.cos(theta1), -torch.sin(theta1), 0, 0],\n",
    "    [torch.sin(theta1),  torch.cos(theta1), 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "R34 = torch.tensor([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [0, 0, torch.cos(theta2), -torch.sin(theta2)],\n",
    "        [0, 0, torch.sin(theta2),  torch.cos(theta2)]\n",
    "])\n",
    "\n",
    "R = torch.mm(R12, R34)\n",
    "\n",
    "R12, R34, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68796984-178f-4d8a-9c80-079360966986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(torch.mm(R, R.T), torch.mm(R.T, R)))\n",
    "print(torch.allclose(torch.mm(R, R.T), torch.eye(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8756e8b4-0773-4af2-8c17-f849619ed1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../../imgs/rope_2.png\" width=\"450\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../../imgs/rope_2.png', width=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01fd84d2-fe50-430f-966e-c27db3a4d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta: m*theta\n",
    "# m: position\n",
    "def get_rope_matrix(d, theta):\n",
    "    \"\"\"Construct the ROPE rotation matrix.\"\"\"\n",
    "    cos_theta = torch.cos(theta)\n",
    "    sin_theta = torch.sin(theta)\n",
    "    mat = torch.zeros(d, d)\n",
    "    mat[0:d//2, 0:d//2] = torch.diag(cos_theta)\n",
    "    mat[d//2:, d//2:] = torch.diag(cos_theta)\n",
    "    mat[0:d//2, d//2:] = -torch.diag(sin_theta)\n",
    "    mat[d//2:, 0:d//2] = torch.diag(sin_theta)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef97a745-aac4-4a9a-9617-a1e0279739b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例向量的维度\n",
    "d = 4  # 必须是偶数\n",
    "theta_m = torch.tensor([i * math.pi / 180 for i in range(d//2)])\n",
    "theta_n = torch.tensor([(i + 1) * math.pi / 180 for i in range(d//2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b00d0d6d-05db-41fe-86d5-404bd73b086b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000, 0.0175]), tensor([0.0175, 0.0349]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_m, theta_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed75338f-dae9-44f0-af7f-984edfdae340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0175, 0.0175])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_n - theta_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e93640a-3f4a-479e-9509-e35e076baeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0175, 0.0175])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1 * math.pi / 180 for i in range(d//2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a259153a-6aa1-454c-93bc-c35c9f31e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造 R_m 和 R_n\n",
    "R_m = get_rope_matrix(d, theta_m)\n",
    "R_n = get_rope_matrix(d, theta_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce2ef054-70de-451c-80a3-93fff05de840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9998,  0.0000, -0.0175, -0.0000],\n",
       "        [ 0.0000,  0.9998, -0.0000, -0.0175],\n",
       "        [ 0.0175,  0.0000,  0.9998,  0.0000],\n",
       "        [ 0.0000,  0.0175,  0.0000,  0.9998]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_m_T = R_m.T\n",
    "R_n_minus_m = get_rope_matrix(d, theta_n - theta_m)\n",
    "R_n_minus_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26cd0839-7de0-4de1-ba3c-99dd02b68086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9998,  0.0000, -0.0175,  0.0000],\n",
       "        [ 0.0000,  0.9998,  0.0000, -0.0175],\n",
       "        [ 0.0175,  0.0000,  0.9998,  0.0000],\n",
       "        [ 0.0000,  0.0175,  0.0000,  0.9998]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = torch.mm(R_m_T, R_n)\n",
    "product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5a3fcdb-78ba-47ea-bd9a-0eb538e562f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试向量 q 和 k\n",
    "q = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "k = torch.tensor([4.0, 3.0, 2.0, 1.0])\n",
    "\n",
    "# 计算 (R_m q)^T (R_n k)\n",
    "R_m_q = torch.mv(R_m, q)\n",
    "R_n_k = torch.mv(R_n, k)\n",
    "# result_1 = torch.dot(R_m_q, R_n_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78d21729-e2e0-470c-9f90-1bda56dd7776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.3460)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算 q^T R_m^T R_n k\n",
    "result_2 = torch.dot(q, torch.mv(product, k))\n",
    "result_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffe523e5-e0ca-403b-8227-476e4a86b53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.3460)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算 q^T R_{n-m} k\n",
    "result_3 = torch.dot(q, torch.mv(R_n_minus_m, k))\n",
    "result_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4950cb5-fb29-46d1-9e3b-8fe0294889a1",
   "metadata": {},
   "source": [
    "## CoPE：Contextual Position Encoding: Learning to Count What’s Important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e02a9fc-7411-4a1b-ad68-f5b339e08d50",
   "metadata": {},
   "source": [
    "- https://arxiv.org/pdf/2405.18719\n",
    "- https://www.zhihu.com/question/657761483/answer/3517582623\n",
    "- 相对位置编码\n",
    "    - 之前的哪怕是 rope，都是基于 token positions 的，独立于上下文\n",
    "    - cope （$p_{ij}$ 的计算已经考虑了 $q_i,k_j$）不只可以 attend 到 token，还可以到 sentence 到 paragraph\n",
    "\n",
    "$$\n",
    "a_{ij}=\\text{Softmax}(\\mathbf q^T_i(\\mathbf k_j+\\mathbf e[i-j]))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3451ee40-3ea7-49d6-9abb-d96f018ad686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../../imgs/cope.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rel p_{ij} 显然是整数\n",
    "# cope, p_{ij} 显然不是整数\n",
    "# 这个例子想说的是我们要关注最后一句话， relative pe 并不能做到很好的 attend 到最后一句话，cope 可以\n",
    "Image(url='../../imgs/cope.png', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a829a362-4938-4ef5-9b24-eb8ea44884aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../../imgs/rel_position_matrix.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../../imgs/rel_position_matrix.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d66c2d-a14b-439a-bbbf-b882987c9fc6",
   "metadata": {},
   "source": [
    "- $g_{ij}=\\sigma(q_i^Tk_j)$（gate value）, $i$ 称为 target，$j\\lt i$, 也就是 $i$ 左边的；\n",
    "    - 可以想象自回归的过程，$i$表示当前位置，$j$则表示过去的已经遍历/生成的 tokens；\n",
    "- $p_{ij}=\\sum_{k=j}^ig_{ik}$\n",
    "    - 如果 $g_{ij}==1$, $p_{ij}=i-j+1$\n",
    "    - 显然 $p_{ij}$ 大概率不是整数，它是我们希望刻画的相对位置；\n",
    "- a learnable embedding vector $\\mathbf e[p]$（$p\\in[0,T]$），做插值；\n",
    "\n",
    "    $$\n",
    "    \\mathbf{e}[p_{ij}] = (p_{ij} - \\lfloor p_{ij} \\rfloor) \\mathbf{e} \\left[ \\lceil p_{ij} \\rceil \\right] + (1 - p_{ij} + \\lfloor p_{ij} \\rfloor) \\mathbf{e} \\left[ \\lfloor p_{ij} \\rfloor \\right].\n",
    "    $$\n",
    "\n",
    "- attention weights\n",
    "\n",
    "    $$\n",
    "    a_{ij}=\\text{Softmax}(\\mathbf q_i^T(\\mathbf k_j+\\mathbf e[p_{ij}]))\n",
    "    $$\n",
    "  - computing and storing vectors $\\mathbf e[p_{ij}]$ uses extra compute and memory.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z_i[p] &= \\mathbf{q}_i^\\top \\mathbf{e}[p] \\quad \\text{for } p \\in \\{0, 1, \\ldots, T\\} \\\\\n",
    "z_i[p_{ij}] &= (p_{ij} - \\lfloor p_{ij} \\rfloor) z_i \\left[ \\lceil p_{ij} \\rceil \\right] + (1 - p_{ij} + \\lfloor p_{ij} \\rfloor) z_i \\left[ \\lfloor p_{ij} \\rfloor \\right] \\\\\n",
    "a_{ij} &= \\text{Softmax}(\\mathbf{q}_i^\\top \\mathbf{k}_j + z_i[p_{ij}]).\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0bfd28-189b-4406-8ee0-3dee8cc1c92b",
   "metadata": {},
   "source": [
    "```\n",
    "class CoPE(nn.Module):\n",
    "    def __init__(self, npos_max, head_dim):\n",
    "        super().__init__()\n",
    "        self.npos_max = npos_max\n",
    "        self.pos_emb = nn.parameter.Parameter(torch.zeros(1, head_dim, npos_max))\n",
    "\n",
    "    def forward(self, query, attn_logits):\n",
    "        # compute positions\n",
    "        gates = torch.sigmoid(attn_logits)\n",
    "        pos = gates.flip(-1).cumsum(dim=-1).flip(-1)\n",
    "        pos = pos.clamp(max=self.npos_max - 1)\n",
    "        # interpolate from integer positions\n",
    "        pos_ceil = pos.ceil().long()\n",
    "        pos_floor = pos.floor().long()\n",
    "        logits_int = torch.matmul(query, self.pos_emb)\n",
    "        logits_ceil = logits_int.gather(-1, pos_ceil)\n",
    "        logits_floor = logits_int.gather(-1, pos_floor)\n",
    "        w = pos - pos_floor\n",
    "        return logits_ceil * w + logits_floor * (1 - w)\n",
    "\n",
    "class SelfAttn(nn.Module):\n",
    "    def __init__(self, npos_max, head_dim):\n",
    "        super().__init__()\n",
    "        self.cope = CoPE(npos_max, head_dim)\n",
    "        self.head_dim = head_dim\n",
    "\n",
    "    def forward(self, query, key, val, mask):\n",
    "        # q, k, v have dimensions batch x seq_len x head_dim\n",
    "        attn_logits = torch.bmm(query, key.transpose(-1, -2))\n",
    "        attn_logits = attn_logits / math.sqrt(self.head_dim)\n",
    "        attn_logits += mask.log()\n",
    "        attn_logits += self.cope(query, attn_logits)\n",
    "        attn = torch.softmax(attn_logits, dim=-1)\n",
    "        out = torch.bmm(attn, val)\n",
    "        return out\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
